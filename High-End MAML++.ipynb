{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from meta_neural_network_architectures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "\n",
    "args = {'num_classes_per_set': 5}\n",
    "args = {'learnable_bn_gamma': True}\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "def filter_dict(key, params_dict):\n",
    "    if params_dict is None:\n",
    "        return None\n",
    "    res_dict = dict()\n",
    "    for name, param in params_dict.items():\n",
    "        bits = name.split('.')\n",
    "        if key in bits:\n",
    "            res_dict['.'.join(bits[1:])] = param\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, device, args, eps=1e-5, momentum=0.1, affine=True,\n",
    "                 track_running_stats=True, meta_batch_norm=True, no_learnable_params=False,\n",
    "                 use_per_step_bn_statistics=False):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.device = device\n",
    "        self.momentum = momentum\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        if params:\n",
    "            weight = params['weight']\n",
    "            bias = params['bias']\n",
    "        else:\n",
    "            weight = self.weight\n",
    "            bias = self.bias\n",
    "\n",
    "        if training:\n",
    "            m1 = x.mean((0, 2, 3))\n",
    "            m2 = (x**2).mean((0, 2, 3))\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * m1\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * (m2 - m1**2)\n",
    "        return F.batch_norm(x, self.running_mean, self.running_var, weight, bias, False,\n",
    "                            self.momentum, self.eps)\n",
    "\n",
    "bn = BatchNorm(4, device, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, *kernel_size))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        if params:\n",
    "            weight = params['weight']\n",
    "            bias = params['bias']\n",
    "        else:\n",
    "            weight = self.weight\n",
    "            bias = self.bias\n",
    "            \n",
    "        return F.conv2d(x, weight, bias=bias, padding=self.padding)\n",
    "            \n",
    "conv = Conv2d(3, 8, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckLayer(nn.Module):\n",
    "    def __init__(self, in_channels, device, args, batch_norm_cls):\n",
    "        super(BottleneckLayer, self).__init__()\n",
    "        self.k = 64 # growth rate\n",
    "        self.bn1 = batch_norm_cls(in_channels, device, args)\n",
    "        self.conv1 = Conv2d(in_channels, 4 * self.k, (1, 1))\n",
    "        self.bn2 = batch_norm_cls(4 * self.k, device, args)\n",
    "        self.conv2 = Conv2d(4 * self.k, self.k, (3, 3), padding=1)\n",
    "    \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        x = self.bn1(x, num_step, params=filter_dict('bn1', params), training=training,\n",
    "                     backup_running_statistics=backup_running_statistics)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x, num_step, params=filter_dict('conv1', params), training=training)\n",
    "        x = self.bn2(x, num_step, params=filter_dict('bn2', params), training=training,\n",
    "                     backup_running_statistics=backup_running_statistics)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, num_step, params=filter_dict('conv2', params), training=training)\n",
    "        return x\n",
    "    \n",
    "bnl = BottleneckLayer(3, 'cpu', args, BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExciteConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SqueezeExciteConvLayer, self).__init__()\n",
    "        reduced = max(in_channels // 16, 1)\n",
    "        self.w1 = nn.Parameter(torch.Tensor(reduced, in_channels))\n",
    "        self.w2 = nn.Parameter(torch.Tensor(in_channels, reduced))\n",
    "\n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        if params:\n",
    "            w1 = params['w1']\n",
    "            w2 = params['w2']\n",
    "        else:\n",
    "            w1 = self.w1\n",
    "            w2 = self.w2\n",
    "        z = x.mean((-2, -1))\n",
    "        F.linear(z, w1)\n",
    "        z = F.relu(F.linear(z, w1))\n",
    "        s = torch.sigmoid(F.linear(z, w2)).unsqueeze(2).unsqueeze(3)\n",
    "        return s * x\n",
    "    \n",
    "se = SqueezeExciteConvLayer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, device, args, batch_norm_cls):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.se1 = SqueezeExciteConvLayer(in_channels)\n",
    "        self.bc1 = BottleneckLayer(in_channels, device, args, batch_norm_cls)\n",
    "        self.se2 = SqueezeExciteConvLayer(self.bc1.k + in_channels)\n",
    "        self.bc2 = BottleneckLayer(self.bc1.k + in_channels, device, args, batch_norm_cls)\n",
    "        self.n_out_channels = self.bc2.k + self.bc1.k + in_channels\n",
    "    \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        y = self.se1(x, num_step, params=filter_dict('se1', params), training=training)\n",
    "        y = self.bc1(x, num_step, params=filter_dict('bc1', params), training=training,\n",
    "                     backup_running_statistics=backup_running_statistics)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        y = self.se2(x, num_step, params=filter_dict('se2', params), training=training)\n",
    "        y = self.bc2(x, num_step, params=filter_dict('bc2', params), training=training,\n",
    "                     backup_running_statistics=backup_running_statistics)\n",
    "        return torch.cat((x, y), 1)\n",
    "    \n",
    "\n",
    "class DenseBlockUnit(nn.Module):\n",
    "    def __init__(self, in_channels, device, args, batch_norm_cls):\n",
    "        super(DenseBlockUnit, self).__init__()\n",
    "        self.se = SqueezeExciteConvLayer(in_channels)\n",
    "        self.bc = BottleneckLayer(in_channels, device, args, batch_norm_cls)\n",
    "        self.n_out_channels = self.bc.k + in_channels\n",
    "    \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        y = self.se(x, num_step, params=filter_dict('se', params), training=training)\n",
    "        y = self.bc(x, num_step, params=filter_dict('bc', params), training=training,\n",
    "                    backup_running_statistics=backup_running_statistics)\n",
    "        return torch.cat((x, y), 1)\n",
    "    \n",
    "db = DenseBlockUnit(3, device, args, BatchNorm)\n",
    "x = torch.randn(10, 3, 6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.8062e-02, -1.8532e-02, -1.4248e-01],\n",
       "          [-7.4135e-02, -2.5804e-02,  9.6382e-02],\n",
       "          [-2.0275e-02, -2.0002e-01, -1.4115e-01]],\n",
       "\n",
       "         [[ 5.9071e-02,  8.6643e-02,  5.7513e-02],\n",
       "          [ 1.4820e-01,  1.8746e-01, -1.4291e-02],\n",
       "          [ 2.2696e-02,  1.1824e-02, -2.9649e-02]],\n",
       "\n",
       "         [[ 6.5036e-02, -7.6815e-06,  4.5541e-02],\n",
       "          [-4.2839e-02, -2.4690e-02,  2.8728e-02],\n",
       "          [ 9.6175e-03, -4.8336e-02, -3.4703e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-4.1704e-03, -6.0966e-03,  6.9034e-03],\n",
       "          [-1.1717e-02,  3.1109e-03,  1.6288e-02],\n",
       "          [-8.2616e-03, -2.0220e-03,  5.8524e-04]],\n",
       "\n",
       "         [[-3.0873e-02, -3.7266e-02, -2.0214e-02],\n",
       "          [-3.8520e-02, -3.0523e-02, -2.3731e-02],\n",
       "          [-2.0387e-02, -7.9308e-03,  1.8661e-03]],\n",
       "\n",
       "         [[-1.3609e-02,  1.1139e-03, -1.5340e-03],\n",
       "          [ 2.8525e-03,  2.2619e-02, -1.6508e-04],\n",
       "          [-4.0847e-03, -2.2121e-03, -1.9427e-02]]],\n",
       "\n",
       "\n",
       "        [[[-6.2242e-02, -1.7743e-02, -3.0795e-02],\n",
       "          [-7.3265e-03, -1.8709e-04, -1.0624e-01],\n",
       "          [-2.0479e-02, -8.4242e-02, -4.5809e-02]],\n",
       "\n",
       "         [[-3.3111e-02,  6.6430e-02,  1.1979e-01],\n",
       "          [ 1.3899e-01,  7.8354e-02,  8.3677e-02],\n",
       "          [ 1.6413e-01, -1.9004e-02, -2.5502e-02]],\n",
       "\n",
       "         [[-2.4636e-02, -1.4816e-01, -1.3982e-02],\n",
       "          [-1.4159e-01, -5.9951e-02, -1.7512e-01],\n",
       "          [-1.5881e-01, -2.3695e-02,  1.0739e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.7560e-02, -1.1090e-02, -3.6722e-03],\n",
       "          [-2.5215e-02, -1.3161e-02, -4.3608e-03],\n",
       "          [-6.6576e-03, -1.2519e-02, -2.3029e-04]],\n",
       "\n",
       "         [[-3.5253e-02, -4.6511e-02, -1.7893e-02],\n",
       "          [-4.1835e-02, -4.2143e-02, -1.3934e-02],\n",
       "          [-1.4436e-02, -1.1280e-02, -1.8187e-03]],\n",
       "\n",
       "         [[-2.2007e-02,  6.7563e-04, -3.8871e-03],\n",
       "          [ 1.0834e-02,  3.1637e-02, -7.2397e-03],\n",
       "          [ 1.5077e-03, -1.2177e-02, -1.5672e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.9563e-02, -5.0035e-02,  2.2283e-02],\n",
       "          [-7.5171e-03, -1.2097e-01,  7.8897e-02],\n",
       "          [ 4.8664e-02, -1.2493e-01,  1.6129e-03]],\n",
       "\n",
       "         [[ 4.8908e-03,  1.4003e-01,  8.2515e-02],\n",
       "          [ 6.8592e-02,  1.2012e-01,  2.9413e-02],\n",
       "          [-3.1683e-02,  5.2890e-02,  5.2544e-02]],\n",
       "\n",
       "         [[-9.3878e-02, -7.6061e-02,  4.6288e-02],\n",
       "          [-3.9002e-02, -2.5815e-02,  7.8786e-02],\n",
       "          [-2.4415e-02, -1.4239e-01,  2.7633e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.1572e-03, -7.6430e-04,  5.1673e-03],\n",
       "          [-1.6529e-02, -1.3953e-02,  9.0962e-03],\n",
       "          [-6.5028e-03, -8.7177e-03,  1.2621e-02]],\n",
       "\n",
       "         [[-3.4758e-02, -3.3396e-02, -2.0687e-02],\n",
       "          [-3.7654e-02, -3.9593e-02, -1.7010e-02],\n",
       "          [-2.9744e-02, -8.0666e-03,  1.0477e-02]],\n",
       "\n",
       "         [[-2.0459e-02, -2.3778e-04, -9.9066e-03],\n",
       "          [-2.7817e-03,  8.1438e-03, -7.9605e-03],\n",
       "          [-2.2186e-03,  2.2105e-03, -9.4420e-03]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 4.0234e-02, -2.3164e-02,  1.6085e-02],\n",
       "          [-1.1796e-02, -6.8855e-02,  4.0405e-02],\n",
       "          [ 1.3858e-03, -4.7509e-02, -9.2822e-02]],\n",
       "\n",
       "         [[ 7.0169e-02,  5.9337e-02,  3.2278e-02],\n",
       "          [ 1.1670e-02,  9.9070e-02, -3.9370e-03],\n",
       "          [ 7.7942e-02,  9.5408e-02,  1.1058e-02]],\n",
       "\n",
       "         [[ 7.5878e-03,  2.6932e-02, -1.0002e-02],\n",
       "          [-7.7036e-02,  5.9236e-02, -1.6014e-01],\n",
       "          [-5.3956e-02, -5.3162e-02, -6.0237e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5229e-02,  2.6997e-03, -4.0622e-03],\n",
       "          [-2.7713e-02, -3.9658e-03, -2.6737e-03],\n",
       "          [-6.7908e-03,  5.1757e-03,  5.0165e-03]],\n",
       "\n",
       "         [[-2.3191e-02, -4.3444e-02, -2.0973e-02],\n",
       "          [-4.1187e-02, -3.9336e-02, -2.2559e-02],\n",
       "          [-2.3613e-02, -4.0149e-03,  4.9846e-03]],\n",
       "\n",
       "         [[-1.4138e-02, -1.4776e-02, -7.7736e-03],\n",
       "          [-3.2025e-03,  1.9082e-02,  2.4764e-03],\n",
       "          [ 3.1535e-03,  9.3635e-03, -1.1453e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.6988e-02,  8.2916e-02, -1.4733e-02],\n",
       "          [ 4.1963e-02, -6.4510e-03, -5.3817e-02],\n",
       "          [-1.9248e-02, -9.7225e-02, -7.1762e-02]],\n",
       "\n",
       "         [[ 8.6964e-02,  1.3230e-01,  1.1680e-01],\n",
       "          [ 8.4389e-02,  6.5579e-02,  8.4821e-03],\n",
       "          [ 7.1422e-02,  1.0920e-02, -1.5507e-02]],\n",
       "\n",
       "         [[-7.5507e-02, -3.5892e-03, -2.5405e-02],\n",
       "          [-3.5657e-02, -1.4511e-02, -7.7417e-02],\n",
       "          [ 8.7173e-02,  9.5408e-03, -5.1393e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5722e-02, -4.7652e-03, -1.6683e-03],\n",
       "          [-8.1361e-03, -5.6052e-03, -7.4512e-03],\n",
       "          [ 2.6525e-03, -4.5970e-03, -7.0147e-05]],\n",
       "\n",
       "         [[-2.7223e-02, -3.9283e-02, -2.6644e-02],\n",
       "          [-3.0223e-02, -3.6021e-02, -3.2018e-02],\n",
       "          [-1.9109e-02, -6.4852e-03,  1.3989e-04]],\n",
       "\n",
       "         [[-1.0694e-02, -1.1132e-03, -1.0821e-02],\n",
       "          [ 5.3948e-03,  1.8353e-02, -1.2604e-02],\n",
       "          [-3.0147e-03, -1.0846e-02, -1.1970e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.2724e-02, -7.5595e-02,  7.0653e-02],\n",
       "          [-1.3572e-02, -4.3748e-02, -1.0422e-01],\n",
       "          [-1.3968e-01, -6.0656e-02, -5.0149e-02]],\n",
       "\n",
       "         [[ 1.1175e-01,  1.2864e-01,  5.7863e-02],\n",
       "          [ 9.1920e-02,  4.0726e-02,  7.0107e-02],\n",
       "          [ 9.2682e-02,  5.1950e-03,  9.2363e-03]],\n",
       "\n",
       "         [[-7.9004e-02, -3.8347e-02,  9.7911e-02],\n",
       "          [-1.6164e-01, -4.9904e-02, -1.7214e-01],\n",
       "          [-9.4593e-02,  7.4332e-02, -9.5091e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.0130e-02, -6.3743e-03,  2.4893e-03],\n",
       "          [-1.1941e-02,  1.2873e-03, -2.5353e-03],\n",
       "          [-9.5814e-03, -6.8066e-03, -6.4493e-03]],\n",
       "\n",
       "         [[-1.7711e-02, -4.8680e-02, -2.8501e-02],\n",
       "          [-4.1606e-02, -4.5527e-02, -2.0262e-02],\n",
       "          [-2.0710e-02, -3.3508e-03,  4.3521e-03]],\n",
       "\n",
       "         [[-1.6590e-02, -1.3975e-02, -4.4947e-03],\n",
       "          [-8.3433e-04,  9.6143e-03, -9.2536e-04],\n",
       "          [-1.0407e-02,  3.0999e-04, -1.0840e-02]]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HighEndEmbedding(nn.Module):\n",
    "    def __init__(self, device, args, in_channels=3):\n",
    "        super(HighEndEmbedding, self).__init__()\n",
    "        \n",
    "        self.dbu1 = DenseBlockUnit(3, device, args, BatchNorm)\n",
    "        self.dbu2 = DenseBlockUnit(self.dbu1.n_out_channels, device, args, BatchNorm)\n",
    "        \n",
    "        n_out2 = max(self.dbu2.n_out_channels // 2, 1)\n",
    "        self.tr_conv = Conv2d(self.dbu2.n_out_channels, n_out2, (1, 1))\n",
    "        self.tr_av_pool = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.dbu3 = DenseBlockUnit(n_out2, device, args, BatchNorm)\n",
    "        self.n_out_channels = self.dbu3.n_out_channels\n",
    "    \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        # First two dense blocks\n",
    "        x = self.dbu1(x, num_step, params=filter_dict('dbu1', params), training=training,\n",
    "                      backup_running_statistics=backup_running_statistics)\n",
    "        x = self.dbu2(x, num_step, params=filter_dict('dbu2', params), training=training,\n",
    "                      backup_running_statistics=backup_running_statistics)\n",
    "        \n",
    "        # Transition\n",
    "        x = self.tr_conv(x, num_step, params=filter_dict('tr_conv', params), training=training)\n",
    "        x = self.tr_av_pool(x) #\n",
    "        \n",
    "        # 3/4:th dense block (embedding)\n",
    "        x = self.dbu3(x, num_step, params=filter_dict('dbu3', params), training=training,\n",
    "                      backup_running_statistics=backup_running_statistics)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HighEndClassifier(nn.Module):\n",
    "    def __init__(self, device, args, in_channels):\n",
    "        super(HighEndClassifier, self).__init__()\n",
    "        self.dbu4 = DenseBlockUnit(in_channels, device, args, MetaBatchNormLayer)\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.dbu4.n_out_channels, args['num_classes_per_set']))\n",
    "        self.bias = nn.Parameter(torch.Tensor(self.dbu4.n_out_channels))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x, num_step, params=None, training=False, backup_running_statistics=False):\n",
    "        x = self.dbu4(x, num_step, params=filter_dict('dbu4', params), training=training,\n",
    "                      backup_running_statistics=backup_running_statistics).mean((-2, -1))\n",
    "        if params:\n",
    "            weight = params['weight']\n",
    "            bias = params['bias']\n",
    "        else:\n",
    "            weight = self.weight\n",
    "            bias = self.bias\n",
    "        x = F.linear(x, weight, bias)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "hee = HighEndEmbedding(device, args)\n",
    "x = torch.randn(10, 3, 6, 6)\n",
    "x = hee(x, 0, params=dict(hee.named_parameters()))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 10, 10)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "he = HighEndClassifierV1({'num_classes_per_set': 5})\n",
    "print(he(x))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "he2 = HighEndClassifierV2({'num_classes_per_set': 5})\n",
    "print(he2(x))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "he3 = HighEndEmbedding()\n",
    "hec3 = HighEndClassifier({'num_classes_per_set': 5}, he3.n_out_channels)\n",
    "print(hec3(he3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
