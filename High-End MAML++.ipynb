{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from meta_neural_network_architectures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, device, args, eps=1e-5, momentum=0.1):\n",
    "        super(BatchNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.weight = nn.Parameter(torch.ones(num_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "        \n",
    "    def forward(self, x, num_step):\n",
    "        if self.training:\n",
    "            m1 = x.mean((0, 2, 3))\n",
    "            m2 = (x**2).mean((0, 2, 3))\n",
    "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * m1\n",
    "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * (m2 - m1**2)\n",
    "        return F.batch_norm(x, self.running_mean, self.running_var, self.weight, self.bias, False,\n",
    "                            self.momentum, self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckLayer(nn.Module):\n",
    "    def __init__(self, in_channels, device, args, batch_norm_cls):\n",
    "        super(BottleneckLayer, self).__init__()\n",
    "        self.k = 64 # growth rateaa\n",
    "        self.bn1 = batch_norm_cls(in_channels, device, args)\n",
    "        self.conv1 = nn.Conv2d(in_channels, 4 * self.k, 1)\n",
    "        self.bn2 = batch_norm_cls(4 * self.k, device, args)\n",
    "        self.conv2 = nn.Conv2d(4 * self.k, self.k, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, num_step):\n",
    "        x = self.conv1(F.relu(self.bn1(x, num_step)))\n",
    "        return self.conv2(F.relu(self.bn2(x, num_step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExciteConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SqueezeExciteConvLayer, self).__init__()\n",
    "        reduced = max(in_channels // 16, 1)\n",
    "        self.w1 = nn.Linear(in_channels, reduced, bias=False)\n",
    "        self.w2 = nn.Linear(reduced, in_channels, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = x.mean((-2, -1))\n",
    "        z = F.relu(self.w1(z))\n",
    "        s = torch.sigmoid(self.w2(z)).unsqueeze(2).unsqueeze(3)\n",
    "        return s * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.se1 = SqueezeExciteConvLayer(in_channels)\n",
    "        self.bc1 = BottleneckLayer(in_channels)\n",
    "        self.se2 = SqueezeExciteConvLayer(self.bc1.k + in_channels)\n",
    "        self.bc2 = BottleneckLayer(self.bc1.k + in_channels)\n",
    "        self.n_out_channels = self.bc2.k + self.bc1.k + in_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.bc1(self.se1(x))\n",
    "        x = torch.cat((x, y), 1)\n",
    "        y = self.bc2(self.se2(x))\n",
    "        return torch.cat((x, y), 1)\n",
    "    \n",
    "\n",
    "class DenseBlockUnit(nn.Module):\n",
    "    def __init__(self, in_channels, device, args, batch_norm_cls):\n",
    "        super(DenseBlockUnit, self).__init__()\n",
    "        self.se = SqueezeExciteConvLayer(in_channels)\n",
    "        self.bc = BottleneckLayer(in_channels, device, args, batch_norm_cls)\n",
    "        self.n_out_channels = self.bc.k + in_channels\n",
    "    \n",
    "    def forward(self, x, num_step):\n",
    "        y = self.bc(self.se(x), num_step)\n",
    "        return torch.cat((x, y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighEndClassifierV1(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(HighEndClassifierV1, self).__init__()\n",
    "        \n",
    "        self.db1 = DenseBlock(3)\n",
    "        n_out2 = max(self.db1.n_out_channels // 2, 1)\n",
    "        self.tr_conv = nn.Conv2d(self.db1.n_out_channels, n_out2, 1)\n",
    "        self.tr_av_pool = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.db2 = DenseBlock(n_out2)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.db2.n_out_channels, args['num_classes_per_set'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.db1(x) # first dense block\n",
    "        x = self.tr_av_pool(self.tr_conv(x)) # transition layer\n",
    "        x = self.db2(x).mean((-2, -1))\n",
    "        return F.softmax(self.lin1(x), dim=-1)\n",
    "\n",
    "\n",
    "class HighEndClassifierV2(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(HighEndClassifierV2, self).__init__()\n",
    "        \n",
    "        self.dbu1 = DenseBlockUnit(3)\n",
    "        self.dbu2 = DenseBlockUnit(self.dbu1.n_out_channels)\n",
    "        \n",
    "        n_out2 = max(self.dbu2.n_out_channels // 2, 1)\n",
    "        self.tr_conv = nn.Conv2d(self.dbu2.n_out_channels, n_out2, 1)\n",
    "        self.tr_av_pool = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.dbu3 = DenseBlockUnit(n_out2)\n",
    "        self.dbu4 = DenseBlockUnit(self.dbu3.n_out_channels)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.dbu4.n_out_channels, args['num_classes_per_set'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dbu2(self.dbu1(x)) # first dense block\n",
    "        x = self.tr_av_pool(self.tr_conv(x)) # transition layer\n",
    "        x = self.dbu4(self.dbu3(x)).mean((-2, -1))\n",
    "        return F.softmax(self.lin1(x), dim=-1)\n",
    "    \n",
    "\n",
    "class HighEndEmbedding(nn.Module):\n",
    "    def __init__(self, device, args):\n",
    "        super(HighEndEmbedding, self).__init__()\n",
    "        \n",
    "        self.dbu1 = DenseBlockUnit(3, device, args, BatchNorm)\n",
    "        self.dbu2 = DenseBlockUnit(self.dbu1.n_out_channels, device, args, BatchNorm)\n",
    "        \n",
    "        n_out2 = max(self.dbu2.n_out_channels // 2, 1)\n",
    "        self.tr_conv = nn.Conv2d(self.dbu2.n_out_channels, n_out2, 1)\n",
    "        self.tr_av_pool = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.dbu3 = DenseBlockUnit(n_out2, device, args, BatchNorm)\n",
    "        self.n_out_channels = self.dbu3.n_out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dbu2(self.dbu1(x)) # first dense block\n",
    "        x = self.tr_av_pool(self.tr_conv(x)) # transition layer\n",
    "        return self.dbu3(x)\n",
    "\n",
    "\n",
    "class HighEndClassifier(nn.Module):\n",
    "    def __init__(self, device, args, in_channels):\n",
    "        super(HighEndClassifier, self).__init__()\n",
    "        \n",
    "        self.dbu4 = DenseBlockUnit(in_channels, MetaBatchNormLayer)\n",
    "        \n",
    "        self.lin1 = nn.Linear(self.dbu4.n_out_channels, args['num_classes_per_set'])\n",
    "    \n",
    "    def forward(self, x, num_step):\n",
    "        x = self.dbu4(x, num_step).mean((-2, -1))\n",
    "        return F.softmax(self.lin1(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2120, 0.1950, 0.1654, 0.2343, 0.1933],\n",
      "        [0.2085, 0.1926, 0.1641, 0.2340, 0.2008]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 10, 10)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "he = HighEndClassifierV1({'num_classes_per_set': 5})\n",
    "print(he(x))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "he2 = HighEndClassifierV2({'num_classes_per_set': 5})\n",
    "print(he2(x))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "he3 = HighEndEmbedding()\n",
    "hec3 = HighEndClassifier({'num_classes_per_set': 5}, he3.n_out_channels)\n",
    "print(hec3(he3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
